##this code runs through a few basic machine learning techniques in an attempt to classify an individual play as "touchdown" vs. "no touchdown"
##primary purpose is to use the "nflscrapr" package
##future goal is to predict first downs instead of touchdowns (avoids the rare cases issue that arises here)


library(nflscrapr)
library(tidyverse)

games_2018 = scrape_game_ids(2018)
pbp_2018 = scrape_season_play_by_play(2018, week = 1)
write.csv(pbp_2018, file = "pbp_2018.csv") #saving data

data = read.csv("C:/Users/ryanh/Documents/pbp_2018.csv")
data$play_type = as.factor(data$play_type)

run_and_pass = data[which(data$play_type == "run" | data$play_type == "pass"),]


newdata = as.data.frame(cbind(touchdown = run_and_pass$touchdown, 
                              type = run_and_pass$play_type, 
                              yards_to_td = run_and_pass$yardline_100, 
                              time = run_and_pass$game_seconds_remaining, 
                              down = run_and_pass$down, 
                              ydstogo = run_and_pass$ydstogo, 
                              goaltogo = run_and_pass$goal_to_go))
newdata = na.omit(newdata)
newdata$touchdown = as.factor(newdata$touchdown)
newdata$type = as.factor(newdata$type)
newdata$yards_to_td = (newdata$yards_to_td - min(newdata$yards_to_td))/(max(newdata$yards_to_td) - min(newdata$yards_to_td))*(2) - 1
newdata$time = (newdata$time - min(newdata$time))/(max(newdata$time) - min(newdata$time))*(2) - 1
newdata$down = as.factor(newdata$down)
newdata$ydstogo = (newdata$ydstogo - min(newdata$ydstogo))/(max(newdata$ydstogo) - min(newdata$ydstogo))*(2) - 1
newdata$goaltogo = as.factor(newdata$goaltogo)


#two separate neural network attempts

#neuralnet

library(neuralnet)
library(sigmoid)

ind = sample(2, nrow(newdata), replace = T, prob = c(0.7, 0.3))
train = newdata[ind == 1,]
test = newdata[ind == 2,]

new.train = model.matrix(~ ., data = train)
new.test = model.matrix(~ ., data = test)

benchmark.accuracy = length(which(train$touchdown == 0))/length(train$touchdown) #assuming we just guessed that all plays were not touchdown

storage = matrix(nrow = 7, ncol = 2)
models = list()
confusion = list()

for(i in 1:7){
  
  model1 = neuralnet(touchdown1 ~ type9 + yards_to_td + time + down2 + down3 + down4 + ydstogo + goaltogo1, 
                   data = new.train, hidden = i, linear.output = FALSE, learningrate = 1e-4)


  pred1 = predict(model1, new.test)
  pred1 = ifelse(pred1 > 0.5, 1, 0)
  conf.mat = table(pred1, test[,1])
  
  test.accuracy = sum(diag(conf.mat))/sum(conf.mat)
  precision = conf.mat[2,2]/(conf.mat[2, 2] + conf.mat[2,1])
  recall = conf.mat[2,2]/(conf.mat[2,2] + conf.mat[1,2])
  f1 = 2*precision*recall/(precision + recall)
  
  models[[i]] = model1
  confusion[[i]] = conf.mat
  acc.storage[i, 1] = i
  acc.storage[i, 2] = test.accuracy
  acc.storage[i, 3] = precision
  acc.storage[i, 4] = recall
  acc.storage[i, 5] = f1

}

report.best = c(accuracy = which.max(acc.storage[,2]), precision = which.max(acc.storage[,3]), recall = which.max(acc.storage[,4]), f1 = which.max(acc.storage[,5]))


#nnet
library(nnet)

acc.storage1 = matrix(nrow = 7, ncol = 5) #choose i to go from 1 to 7 to avoid overfitting since we only have 7 inputs
models1 = list()
confusion1 = list()

for(i in 1:7){
  
  model1 = nnet(touchdown  ~ ., 
                     data = train, size = i, maxit = 1000, decay = 1e-4)
  
  
  pred1 = predict(model1, test)
  pred1 = ifelse(pred1 > 0.5, 1, 0)
  conf.mat = table(pred1, test[,1])
  
  test.accuracy = sum(diag(conf.mat))/sum(conf.mat)
  precision = conf.mat[2,2]/(conf.mat[2, 2] + conf.mat[2,1])
  recall = conf.mat[2,2]/(conf.mat[2,2] + conf.mat[1,2])
  f1 = 2*precision*recall/(precision + recall)
  
  models1[[i]] = model1
  confusion1[[i]] = conf.mat
  acc.storage1[i, 1] = i
  acc.storage1[i, 2] = test.accuracy
  acc.storage1[i, 3] = precision
  acc.storage1[i, 4] = recall
  acc.storage1[i, 5] = f1

}

report.best1 = c(accuracy = which.max(acc.storage1[,2]), precision = which.max(acc.storage1[,3]), recall = which.max(acc.storage1[,4]), f1 = which.max(acc.storage1[,5]))

#since we care most about recall, we could make the decision to use n = 5 nodes.

##randomforest

library(randomForest)
library(caret)

rf = randomForest(touchdown ~ ., data = train, 
                  ntree = 100, 
                  mtry = 2,
                  importance = TRUE,
                  proximity = TRUE)

p1 = predict(rf, test)
conf.rf = table(p1, test$touchdown)

t = tuneRF(train[,-1], train[,1],
       stepFactor = 0.5,
       plot = TRUE,
       ntreeTry = 50,
       trace = TRUE,
       improve = 0.01)

#performs VERY poorly, worse than assuming all cases are false/no touchdown
#need to address rare case problem using gradient boosting (coming soon)
